{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, eps=1e-6):\n",
    "    # LayerNorm(x + Sublayer(x))\n",
    "    feature_shape = inputs.get_shape()[-1:]\n",
    "    #  평균과 표준편차을 넘겨 준다.\n",
    "    mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\n",
    "    std = tf.keras.backend.std(inputs, [-1], keepdims=True)\n",
    "    beta = tf.get_variable(\"beta\", initializer=tf.zeros(feature_shape))\n",
    "    gamma = tf.get_variable(\"gamma\", initializer=tf.ones(feature_shape))\n",
    "\n",
    "    return gamma * (inputs - mean) / (std + eps) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublayer_connection(inputs, sublayer, dropout=0.2):\n",
    "    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(dim, sentence_length):\n",
    "    encoded_vec = np.array([pos/np.power(10000, 2*i/dim)\n",
    "                            for pos in range(sentence_length) for i in range(dim)])\n",
    "\n",
    "    encoded_vec[::2] = np.sin(encoded_vec[::2])\n",
    "    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n",
    "\n",
    "    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.Model): # In 2.0 tf.keras.Model => tf.layers.Layer\n",
    "    def __init__(self, num_units, heads, masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = heads\n",
    "        self.sub_masked = sub_masked\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "        self.key_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "        self.value_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "        self.out_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, key_mask=None):\n",
    "        key_seq_length = float(key.get_shape().as_list()[-1])\n",
    "        key = tf.transpose(key, perm=[0, 2, 1])\n",
    "        outputs = tf.matmul(query, key) / tf.sqrt(key_seq_length)\n",
    "\n",
    "        if self.sub_masked:                \n",
    "            diag_vals = tf.ones_like(outputs[0, :, :])\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n",
    "            \n",
    "            if key_mask is not None:\n",
    "                masks = tf.cast(tf.logical_and(tf.cast(masks, tf.bool),\n",
    "                                              tf.cast(tf.expand_dims(key_mask, 1), tf.bool)),\n",
    "                               tf.float32)\n",
    "\n",
    "            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n",
    "\n",
    "        attention_map = tf.nn.softmax(outputs)\n",
    "\n",
    "        return tf.matmul(attention_map, value)\n",
    "\n",
    "    def call(self, query, key, value, key_mask):\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        query = tf.concat(tf.split(query, self.heads, axis=-1), axis=0)\n",
    "        key = tf.concat(tf.split(key, self.heads, axis=-1), axis=0)\n",
    "        value = tf.concat(tf.split(value, self.heads, axis=-1), axis=0)\n",
    "\n",
    "        attention_map = self.scaled_dot_product_attention(query, key, value, key_mask)\n",
    "\n",
    "        attn_outputs = tf.concat(tf.split(attention_map, self.heads, axis=0), axis=-1)\n",
    "\n",
    "        return self.out_dense(attn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.Model):\n",
    "    def __init__(self, num_units, feature_shape):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.inner_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.output_dense = tf.keras.layers.Dense(feature_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inner_layer = self.inner_dense(inputs)\n",
    "        outputs = self.output_dense(inner_layer)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.self_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\n",
    "        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs, src_mask):\n",
    "        output_layer = None\n",
    "\n",
    "        for i, (s_a, p_f) in enumerate(zip(self.self_attention, self.position_feedforward)):\n",
    "            with tf.variable_scope('encoder_layer_' + str(i + 1)):\n",
    "                attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs, src_mask))\n",
    "                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\n",
    "\n",
    "                inputs = output_layer\n",
    "\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.self_attention = [MultiHeadAttention(model_dims, attn_heads, masked=True) for _ in range(num_layers)]\n",
    "        self.encoder_decoder_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\n",
    "        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, src_mask, tgt_mask):\n",
    "        output_layer = None\n",
    "\n",
    "        for i, (s_a, ed_a, p_f) in enumerate(zip(self.self_attention, self.encoder_decoder_attention, self.position_feedforward)):\n",
    "            with tf.variable_scope('decoder_layer_' + str(i + 1)):\n",
    "                masked_attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs, tgt_mask))\n",
    "                attention_layer = sublayer_connection(masked_attention_layer, ed_a(masked_attention_layer,\n",
    "                                                                                           encoder_outputs,\n",
    "                                                                                           encoder_outputs, src_mask))\n",
    "                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\n",
    "                inputs = output_layer\n",
    "\n",
    "        return output_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
