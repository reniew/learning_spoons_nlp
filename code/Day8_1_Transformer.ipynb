{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './gdrive/My Drive/Colab Notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_path = base_path + '/data_in/'\n",
    "data_out_path = base_path + '/data_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_in_path):\n",
    "    os.makedirs(data_in_path)\n",
    "    \n",
    "if not os.path.exists(data_out_path):\n",
    "    os.makedirs(data_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-16 15:11:48--  https://raw.githubusercontent.com/changwookjun/learningspoons/master/Data/ChatBotData.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.228.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.228.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 889842 (869K) [text/plain]\n",
      "Saving to: ‘./data_in/ChatBotData.csv’\n",
      "\n",
      "ChatBotData.csv     100%[===================>] 868.99K  1.16MB/s    in 0.7s    \n",
      "\n",
      "2019-10-16 15:11:50 (1.16 MB/s) - ‘./data_in/ChatBotData.csv’ saved [889842/889842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P /content/gdrive/My\\ Drive/Colab\\ Notebooks/data_in/ https://raw.githubusercontent.com/changwookjun/learningspoons/master/Data/ChatBotData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_in_path + 'ChatBotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12시 땡!', '하루가 또 가네요.'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Q','A']].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = []\n",
    "\n",
    "for line in data[['Q','A']].values:\n",
    "    question, answer = line\n",
    "    string_list.append(question + '\\t' + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12시 땡!\\t하루가 또 가네요.',\n",
       " '1지망 학교 떨어졌어\\t위로해 드립니다.',\n",
       " '3박4일 놀러가고 싶다\\t여행은 언제나 좋죠.',\n",
       " '3박4일 정도 놀러가고 싶다\\t여행은 언제나 좋죠.',\n",
       " 'PPL 심하네\\t눈살이 찌푸려지죠.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: text 로 train,test저장하는 것부터 수정 필\n",
    "train_data, test_data = train_test_split(string_list, test_size = 0.1)\n",
    "\n",
    "train_string_data = '\\n'.join(train_data)\n",
    "test_string_data = '\\n'.join(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_in_path + 'train_chat_data.txt'\n",
    "test_path = data_in_path + 'test_chat_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, 'w') as f:\n",
    "    f.writelines(train_string_data)\n",
    "with open(test_path, 'w') as f:\n",
    "    f.writelines(test_string_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(train_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'남자친구하고 커플티를 하고싶은데 어디거를 할까?\\t좋아하는 브랜드가 좋겠어요.\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, train_path, test_path, is_shuffle, train_bs,\n",
    "                 test_bs, epoch, max_length, vocab_path):\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.is_shuffle = is_shuffle\n",
    "        self.train_bs = train_bs\n",
    "        self.test_bs = test_bs\n",
    "        self.epoch = epoch\n",
    "        self.max_length = max_length\n",
    "        self.okt = Okt()\n",
    "        self.special_tokens = ['<PAD>', '<BOS>', '<EOS>']\n",
    "        \n",
    "        if not os.path.exists(vocab_path):\n",
    "            print('There is no vocabulary...')\n",
    "            print('Building vocabulary...')\n",
    "            self.build_vocab_by_chatdata(vocab_path)\n",
    "            print('Successfully build vocabulary!')\n",
    "        \n",
    "        print('Loading vocabulary...')    \n",
    "        self.idx2word, self.word2idx = pickle.load(open(vocab_path, 'rb'))\n",
    "        print('Successfully load vocabulary!')\n",
    "    \n",
    "    def build_vocab(self, word_list):\n",
    "        from collections import Counter\n",
    "\n",
    "        word_counts = Counter(word_list)\n",
    "        idx2word = self.special_tokens + [word for word, _ in word_counts.most_common()]\n",
    "        word2idx = {word:idx for idx, word in enumerate(idx2word)}\n",
    "\n",
    "        return idx2word, word2idx\n",
    "    \n",
    "    def build_vocab_by_chatdata(self, vocab_path):\n",
    "        with open(self.train_path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "        \n",
    "        questions = []\n",
    "        answers = []\n",
    "        \n",
    "        for line in data:\n",
    "            string = line.replace('\\n', '')\n",
    "            question, answer = string.split('\\t')\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "        questions = self.tokenize_by_morph(questions)\n",
    "        answers = self.tokenize_by_morph(answers)\n",
    "        \n",
    "        word_list = sum(questions+answers, [])\n",
    "        idx2word, word2idx = self.build_vocab(word_list)\n",
    "        \n",
    "        vocab = (idx2word, word2idx)\n",
    "        pickle.dump(vocab, open(vocab_path, 'wb'))\n",
    "        \n",
    "    def tokenize_by_morph(self, text):\n",
    "        tokenized_text = []\n",
    "        for sentence in text:\n",
    "            tokenized_text.append(self.okt.morphs(sentence))\n",
    "\n",
    "        return tokenized_text\n",
    "    \n",
    "    def text_to_sequence(self, text_list):\n",
    "        sequences = []\n",
    "        for text in text_list:\n",
    "            sequences.append([self.word2idx[word] for word in text if word in self.word2idx.keys()])\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        \n",
    "        return [self.idx2word[idx] for idx in sequence if idx != 0]\n",
    "    \n",
    "    def make_decoder_input_and_label(self, answers):\n",
    "        \n",
    "        decoder_input = []\n",
    "        labels = []\n",
    "        \n",
    "        for sentence in answers:\n",
    "            decoder_input.append(['<BOS>'] + sentence)\n",
    "            labels.append(sentence + ['<EOS>'])\n",
    "        \n",
    "        return decoder_input, labels\n",
    "            \n",
    "    \n",
    "    def read_lines(self, indices, path):\n",
    "        line_count = 0\n",
    "        questions = []\n",
    "        answers = []\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                if line_count in indices:\n",
    "                    string = line.replace('\\n', '')\n",
    "                    question, answer = string.split('\\t') \n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "                line_count += 1\n",
    "\n",
    "        return questions, answers\n",
    "\n",
    "    def data_generator(self, is_train):\n",
    "\n",
    "        if is_train:\n",
    "            batch_size = self.train_bs\n",
    "            is_shuffle = self.is_shuffle\n",
    "            path = self.train_path\n",
    "        else:\n",
    "            batch_size = self.test_bs\n",
    "            is_shuffle = False\n",
    "            path = self.test_path\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            data_length = len(f.readlines())\n",
    "\n",
    "        indices = list(range(data_length))\n",
    "        if is_shuffle:\n",
    "            shuffle(indices)\n",
    "\n",
    "        current_count = 0\n",
    "        while True:\n",
    "            if current_count >= data_length:\n",
    "                return\n",
    "            else:\n",
    "                target_indices = indices[current_count:current_count+batch_size]\n",
    "                current_count += batch_size\n",
    "                questions, answers = self.read_lines(target_indices, path)\n",
    "\n",
    "                tokenized_questions = self.tokenize_by_morph(questions)\n",
    "                tokenized_answers = self.tokenize_by_morph(answers)\n",
    "                \n",
    "                tokenized_encoder_inputs = tokenized_questions\n",
    "                tokenized_decoder_inputs, tokenized_labels = self.make_decoder_input_and_label(tokenized_answers)\n",
    "                \n",
    "\n",
    "                indexed_encoder_inputs = self.text_to_sequence(tokenized_encoder_inputs)\n",
    "                indexed_decoder_inputs = self.text_to_sequence(tokenized_decoder_inputs)\n",
    "                indexed_labels = self.text_to_sequence(tokenized_labels)\n",
    "\n",
    "\n",
    "                padded_encoder_inputs = pad_sequences(indexed_encoder_inputs,\n",
    "                                                      maxlen = self.max_length,\n",
    "                                                      padding = 'pre')\n",
    "                padded_decoder_inputs = pad_sequences(indexed_decoder_inputs,\n",
    "                                                      maxlen = self.max_length,\n",
    "                                                      padding = 'pre')\n",
    "\n",
    "                padded_labels = pad_sequences(indexed_labels,\n",
    "                                              maxlen = self.max_length,\n",
    "                                              padding = 'pre')\n",
    "\n",
    "                \n",
    "                yield padded_encoder_inputs, padded_decoder_inputs, padded_labels\n",
    "\n",
    "    \n",
    "    def mapping_fn(self, question, answer, labels=None):\n",
    "        features = {\"question\": question, 'answer': answer}\n",
    "\n",
    "        return features, labels\n",
    "    \n",
    "    def train_input_fn(self):\n",
    "        dataset = tf.data.Dataset.from_generator(generator = lambda: self.data_generator(is_train=True),\n",
    "                                                output_types = (tf.int64, tf.int64, tf.int64),\n",
    "                                                output_shapes = ((None, self.max_length),\n",
    "                                                                 (None, self.max_length),\n",
    "                                                                 (None, self.max_length)))\n",
    "        dataset = dataset.map(self.mapping_fn)\n",
    "        dataset = dataset.repeat(self.epoch)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def test_input_fn(self):\n",
    "        dataset = tf.data.Dataset.from_generator(generator = lambda: self.data_generator(is_train=False),\n",
    "                                                output_types = (tf.int64, tf.int64),\n",
    "                                                output_shapes = ((None, self.max_length),\n",
    "                                                                 (None, self.max_length)))\n",
    "        dataset = dataset.map(self.mapping_fn)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no vocabulary...\n",
      "Building vocabulary...\n",
      "Successfully build vocabulary!\n",
      "Loading vocabulary...\n",
      "Successfully load vocabulary!\n"
     ]
    }
   ],
   "source": [
    "vocab_path = data_in_path+'ChatBotData.voc'\n",
    "dataset = Dataset(train_path = train_path,\n",
    "                  test_path = test_path,\n",
    "                  is_shuffle = True,\n",
    "                  train_bs = 64,\n",
    "                  test_bs = 128,\n",
    "                  epoch = 10,\n",
    "                  max_length = 30,\n",
    "                  vocab_path = vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, eps=1e-6):\n",
    "    # LayerNorm(x + Sublayer(x))\n",
    "    feature_shape = inputs.get_shape()[-1:]\n",
    "    #  평균과 표준편차을 넘겨 준다.\n",
    "    mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\n",
    "    std = tf.keras.backend.std(inputs, [-1], keepdims=True)\n",
    "    beta = tf.get_variable(\"beta\", initializer=tf.zeros(feature_shape))\n",
    "    gamma = tf.get_variable(\"gamma\", initializer=tf.ones(feature_shape))\n",
    "\n",
    "    return gamma * (inputs - mean) / (std + eps) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublayer_connection(inputs, sublayer, dropout=0.2):\n",
    "    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(dim, sentence_length):\n",
    "    encoded_vec = np.array([pos/np.power(10000, 2*i/dim)\n",
    "                            for pos in range(sentence_length) for i in range(dim)])\n",
    "\n",
    "    encoded_vec[::2] = np.sin(encoded_vec[::2])\n",
    "    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n",
    "\n",
    "    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a35e30b57939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_masked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, heads, sub_masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = heads\n",
    "        self.sub_masked = sub_masked\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "        self.key_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "        self.value_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "        self.out_dense = tf.keras.layers.Dense(num_units, use_bias=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, key_mask=None):\n",
    "        key_seq_length = float(key.get_shape().as_list()[-1])\n",
    "        key = tf.transpose(key, perm=[0, 2, 1])\n",
    "        outputs = tf.matmul(query, key) / tf.sqrt(key_seq_length)\n",
    "        \n",
    "        masks = tf.ones_like(outputs)\n",
    "        masks = tf.cast(tf.logical_and(tf.cast(masks, tf.bool),\n",
    "                                      tf.cast(tf.expand_dims(key_mask, 1), tf.bool)),\n",
    "                       tf.float32)\n",
    "        if self.sub_masked:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :])\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
    "            subsequent_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n",
    "            masks = tf.cast(tf.logical_and(tf.cast(masks, tf.bool),\n",
    "                                          tf.cast(subsequent_masks, tf.bool)),\n",
    "                           tf.float32)\n",
    "        inf = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
    "        outputs = tf.where(tf.equal(masks, 0), inf, outputs)\n",
    "\n",
    "        attention_map = tf.nn.softmax(outputs)\n",
    "\n",
    "        return tf.matmul(attention_map, value)\n",
    "\n",
    "    def call(self, query, key, value, key_mask):\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        query = tf.concat(tf.split(query, self.heads, axis=-1), axis=0)\n",
    "        key = tf.concat(tf.split(key, self.heads, axis=-1), axis=0)\n",
    "        value = tf.concat(tf.split(value, self.heads, axis=-1), axis=0)\n",
    "\n",
    "        attention_map = self.scaled_dot_product_attention(query, key, value, key_mask)\n",
    "\n",
    "        attn_outputs = tf.concat(tf.split(attention_map, self.heads, axis=0), axis=-1)\n",
    "\n",
    "        return self.out_dense(attn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, feature_shape):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.inner_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.output_dense = tf.keras.layers.Dense(feature_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inner_layer = self.inner_dense(inputs)\n",
    "        outputs = self.output_dense(inner_layer)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.self_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\n",
    "        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs, src_mask):\n",
    "        output_layer = None\n",
    "\n",
    "        for i, (s_a, p_f) in enumerate(zip(self.self_attention, self.position_feedforward)):\n",
    "            with tf.variable_scope('encoder_layer_' + str(i + 1)):\n",
    "                attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs, src_mask))\n",
    "                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\n",
    "\n",
    "                inputs = output_layer\n",
    "\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.self_attention = [MultiHeadAttention(model_dims, attn_heads, sub_masked=True) for _ in range(num_layers)]\n",
    "        self.encoder_decoder_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\n",
    "        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, src_mask, tgt_mask):\n",
    "        output_layer = None\n",
    "\n",
    "        for i, (s_a, ed_a, p_f) in enumerate(zip(self.self_attention, self.encoder_decoder_attention, self.position_feedforward)):\n",
    "            with tf.variable_scope('decoder_layer_' + str(i + 1)):\n",
    "                masked_attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs, tgt_mask))\n",
    "                attention_layer = sublayer_connection(masked_attention_layer, ed_a(masked_attention_layer,\n",
    "                                                                                           encoder_outputs,\n",
    "                                                                                           encoder_outputs, src_mask))\n",
    "                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\n",
    "                inputs = output_layer\n",
    "\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_function(features, labels, mode, params):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    src_mask = tf.not_equal(features['question'], 0)\n",
    "    tgt_mask = tf.not_equal(features['answer'], 0)\n",
    "    \n",
    "    src_mask = tf.tile(src_mask, [params['attention_head_size'], 1])\n",
    "    tgt_mask = tf.tile(tgt_mask, [params['attention_head_size'], 1])\n",
    "\n",
    "    position_encode = positional_encoding(params['model_hidden_size'], params['max_sequence_length'])\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(params['vocabulary_length'],\n",
    "                                          params['model_hidden_size'])\n",
    "\n",
    "    encoder_layers = Encoder(params['model_hidden_size'], params['ffn_hidden_size'],\n",
    "                      params['attention_head_size'], params['layer_size'])\n",
    "\n",
    "    decoder_layers = Decoder(params['model_hidden_size'], params['ffn_hidden_size'],\n",
    "                      params['attention_head_size'], params['layer_size'])\n",
    "\n",
    "    logit_layer = tf.keras.layers.Dense(params['vocabulary_length'])\n",
    "\n",
    "    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n",
    "        x_embedded_matrix = embedding(features['question']) + position_encode\n",
    "        encoder_outputs = encoder_layers(x_embedded_matrix, src_mask)\n",
    "\n",
    "    loop_count = params['max_sequence_length'] if PREDICT else 1\n",
    "\n",
    "    predict, output, logits = None, None, None\n",
    "\n",
    "    for i in range(loop_count):\n",
    "        with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):\n",
    "            if i > 0:\n",
    "                output = tf.concat([tf.ones((output.shape[0], 1), dtype=tf.int64), predict[:, :-1]], axis=-1)\n",
    "            else:\n",
    "                output = features['answer']\n",
    "\n",
    "            y_embedded_matrix = embedding(output) + position_encode\n",
    "            decoder_outputs = decoder_layers(y_embedded_matrix, encoder_outputs, src_mask, tgt_mask)\n",
    "\n",
    "            logits = logit_layer(decoder_outputs)\n",
    "            predict = tf.argmax(logits, 2)\n",
    "\n",
    "    if PREDICT:\n",
    "        predictions = {\n",
    "            'indexs': predict,\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict, name='accOp')\n",
    "\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    assert TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {'vocabulary_length': len(dataset.word2idx),\n",
    "                'embedding_size': 128,\n",
    "                'model_hidden_size': 128,\n",
    "                'ffn_hidden_size': 128*4,\n",
    "                'layer_size': 3,\n",
    "                'attention_head_size': 8,\n",
    "                'max_sequence_length': 30,\n",
    "                'learning_rate': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 21:04:50.374693 4554216896 estimator.py:1790] Using default config.\n",
      "I1016 21:04:50.375674 4554216896 estimator.py:209] Using config: {'_model_dir': './data_out/transformer', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8b265fc390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn = model_function,\n",
    "                                   params=hyper_params,\n",
    "                                   model_dir =data_out_path+'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.train(dataset.train_input_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
